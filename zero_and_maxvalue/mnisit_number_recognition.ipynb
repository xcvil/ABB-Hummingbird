{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ananonda\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(30, (5, 5), input_shape=(28, 28, 1..., activation=\"relu\", padding=\"valid\")`\n",
      "D:\\ananonda\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(15, (3, 3), activation=\"relu\")`\n",
      "D:\\ananonda\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ananonda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      " - 29s - loss: 0.9373 - accuracy: 0.6809 - val_loss: 0.1545 - val_accuracy: 0.9551\n",
      "Epoch 2/200\n",
      " - 27s - loss: 0.3173 - accuracy: 0.9085 - val_loss: 0.0897 - val_accuracy: 0.9731\n",
      "Epoch 3/200\n",
      " - 28s - loss: 0.2417 - accuracy: 0.9322 - val_loss: 0.0718 - val_accuracy: 0.9775\n",
      "Epoch 4/200\n",
      " - 28s - loss: 0.1967 - accuracy: 0.9449 - val_loss: 0.0614 - val_accuracy: 0.9813\n",
      "Epoch 5/200\n",
      " - 27s - loss: 0.1794 - accuracy: 0.9517 - val_loss: 0.0521 - val_accuracy: 0.9843\n",
      "Epoch 6/200\n",
      " - 27s - loss: 0.1592 - accuracy: 0.9566 - val_loss: 0.0490 - val_accuracy: 0.9841\n",
      "Epoch 7/200\n",
      " - 28s - loss: 0.1449 - accuracy: 0.9595 - val_loss: 0.0443 - val_accuracy: 0.9870\n",
      "Epoch 8/200\n",
      " - 27s - loss: 0.1353 - accuracy: 0.9639 - val_loss: 0.0420 - val_accuracy: 0.9879\n",
      "Epoch 9/200\n",
      " - 27s - loss: 0.1254 - accuracy: 0.9655 - val_loss: 0.0409 - val_accuracy: 0.9879\n",
      "Epoch 10/200\n",
      " - 29s - loss: 0.1206 - accuracy: 0.9675 - val_loss: 0.0391 - val_accuracy: 0.9891\n",
      "Epoch 11/200\n",
      " - 27s - loss: 0.1174 - accuracy: 0.9675 - val_loss: 0.0358 - val_accuracy: 0.9891\n",
      "Epoch 12/200\n",
      " - 27s - loss: 0.1088 - accuracy: 0.9710 - val_loss: 0.0338 - val_accuracy: 0.9901\n",
      "Epoch 13/200\n",
      " - 28s - loss: 0.1037 - accuracy: 0.9718 - val_loss: 0.0329 - val_accuracy: 0.9897\n",
      "Epoch 14/200\n",
      " - 28s - loss: 0.1052 - accuracy: 0.9715 - val_loss: 0.0319 - val_accuracy: 0.9909\n",
      "Epoch 15/200\n",
      " - 26s - loss: 0.0998 - accuracy: 0.9733 - val_loss: 0.0307 - val_accuracy: 0.9907\n",
      "Epoch 16/200\n",
      " - 28s - loss: 0.0994 - accuracy: 0.9734 - val_loss: 0.0292 - val_accuracy: 0.9915\n",
      "Epoch 17/200\n",
      " - 27s - loss: 0.0933 - accuracy: 0.9747 - val_loss: 0.0301 - val_accuracy: 0.9907\n",
      "Epoch 18/200\n",
      " - 26s - loss: 0.0948 - accuracy: 0.9746 - val_loss: 0.0299 - val_accuracy: 0.9911\n",
      "Epoch 19/200\n",
      " - 26s - loss: 0.0915 - accuracy: 0.9752 - val_loss: 0.0274 - val_accuracy: 0.9919\n",
      "Epoch 20/200\n",
      " - 26s - loss: 0.0853 - accuracy: 0.9773 - val_loss: 0.0271 - val_accuracy: 0.9922\n",
      "Epoch 21/200\n",
      " - 27s - loss: 0.0850 - accuracy: 0.9766 - val_loss: 0.0271 - val_accuracy: 0.9917\n",
      "Epoch 22/200\n",
      " - 26s - loss: 0.0826 - accuracy: 0.9776 - val_loss: 0.0280 - val_accuracy: 0.9915\n",
      "Epoch 23/200\n",
      " - 26s - loss: 0.0811 - accuracy: 0.9778 - val_loss: 0.0255 - val_accuracy: 0.9921\n",
      "Epoch 24/200\n",
      " - 26s - loss: 0.0798 - accuracy: 0.9777 - val_loss: 0.0252 - val_accuracy: 0.9922\n",
      "Epoch 25/200\n",
      " - 26s - loss: 0.0782 - accuracy: 0.9787 - val_loss: 0.0243 - val_accuracy: 0.9926\n",
      "Epoch 26/200\n",
      " - 26s - loss: 0.0806 - accuracy: 0.9779 - val_loss: 0.0233 - val_accuracy: 0.9930\n",
      "Epoch 27/200\n",
      " - 26s - loss: 0.0806 - accuracy: 0.9782 - val_loss: 0.0246 - val_accuracy: 0.9929\n",
      "Epoch 28/200\n",
      " - 26s - loss: 0.0770 - accuracy: 0.9781 - val_loss: 0.0240 - val_accuracy: 0.9930\n",
      "Epoch 29/200\n",
      " - 26s - loss: 0.0736 - accuracy: 0.9798 - val_loss: 0.0237 - val_accuracy: 0.9928\n",
      "Epoch 30/200\n",
      " - 27s - loss: 0.0731 - accuracy: 0.9800 - val_loss: 0.0262 - val_accuracy: 0.9915\n",
      "Epoch 31/200\n"
     ]
    }
   ],
   "source": [
    "# Larger CNN for the MNIST Dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28,28,1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28,28,1).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(30, 5, 5, border_mode='valid', input_shape=(28,28,1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Convolution2D(15, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=200, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('m_lenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数字：\n",
      "识别为：\n",
      "[2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "model = load_model('m_lenet.h5')\n",
    "\n",
    "def pre_pic(picName):\n",
    "    # 先打开传入的原始图片\n",
    "    img = Image.open(picName)\n",
    "    # 使用消除锯齿的方法resize图片\n",
    "    reIm = img.resize((28,28),Image.ANTIALIAS)\n",
    "    # 变成灰度图，转换成矩阵\n",
    "    im_arr = np.array(reIm.convert(\"L\"))\n",
    "    return im_arr\n",
    "\n",
    "im1 = pre_pic('2.png')\n",
    "print('输入数字：')\n",
    "\n",
    "plt.imshow(im1,cmap=plt.get_cmap('gray'))\n",
    "plt.show\n",
    "\n",
    "im1 = im1.reshape((1,28*28))\n",
    "im1 = im1.astype('float32')/255\n",
    "\n",
    "predict = model.predict_classes(im1)\n",
    "print ('识别为：')\n",
    "print (predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
